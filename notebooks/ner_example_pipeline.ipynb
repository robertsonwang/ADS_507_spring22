{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a26baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r ner_example_requirements\n",
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b588e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import praw\n",
    "import spacy\n",
    "import sqlite3\n",
    "\n",
    "from hashlib import md5 \n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8b9e4e",
   "metadata": {},
   "source": [
    "# Named Entity Recognition with Spacy\n",
    "\n",
    "See this [blog post](https://towardsdatascience.com/named-entity-recognition-ner-using-spacy-nlp-part-4-28da2ece57c6) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER Model\n",
    "ner_model = spacy.load(\"en_core_web_sm\", \n",
    "                       disable=[\"tok2vec\", \"tagger\", \"parser\", \n",
    "                                \"attribute_ruler\", \"lemmatizer\"]\n",
    "                      )\n",
    "\n",
    "# Mak example\n",
    "doc = ner_model(\"Apple is looking to buy UK start up for $1 billion dollars\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text)\n",
    "    print(ent.label_)\n",
    "    print(spacy.explain(ent.label_))\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_predictions(text, ner_model, label):\n",
    "    doc = ner_model(text)\n",
    "    predictions = [ent.text for ent in doc.ents if ent.label_ == label]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9cfdc",
   "metadata": {},
   "source": [
    "# Extract - Scrape Reddit Data\n",
    "\n",
    "In order to get your own credentials, please follow the instructions laid out in [this blog post](https://towardsdatascience.com/scraping-reddit-data-1c0af3040768)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f8bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=os.environ[\"REDDIT_CLIENT_ID\"],\n",
    "                     client_secret=os.environ[\"REDDIT_CLIENT_SECRET\"],\n",
    "                     user_agent=os.environ[\"REDDIT_USER_AGENT\"],\n",
    "                     username=os.environ[\"REDDIT_USERNAME\"],\n",
    "                     password=os.environ[\"REDDIT_PASSWORD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5726d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit(\"nyc\")\n",
    "\n",
    "top_posts = list(subreddit.top(limit=50))\n",
    "\n",
    "comments = []\n",
    "users = []\n",
    "title = []\n",
    "submission_id = []\n",
    "time = []\n",
    "for submission in tqdm(top_posts):\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    for comment in submission.comments.list():\n",
    "        if not comment.author:\n",
    "            continue\n",
    "        comments.append(comment.body)\n",
    "        users.append(comment.author.name)\n",
    "        title.append(submission.title)\n",
    "        submission_id.append(submission.name)\n",
    "        time.append(comment.created_utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9fcc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe and persist data\n",
    "extracted_reddit_data = pd.DataFrame(\n",
    "    {'comment': comments,\n",
    "     'user': users,\n",
    "     'post': title,\n",
    "     'time': time}\n",
    ")\n",
    "extracted_reddit_data['hash'] = extracted_reddit_data.comment.apply(lambda x: md5(x.encode()).hexdigest())\n",
    "extracted_reddit_data.to_csv(\"extracted_reddit_data_nyc_2021_01_17.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfd912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cached Data\n",
    "extracted_reddit_data = pd.read_csv(\"extracted_reddit_data_nyc_2021_01_17.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb5c885",
   "metadata": {},
   "source": [
    "# Transform - Get NER Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9dc989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_entities, users, hashes = [], [], []\n",
    "\n",
    "for _, row in tqdm(extracted_reddit_data.iterrows(), \n",
    "                   total=len(extracted_reddit_data)):\n",
    "    text = row['comment']\n",
    "    user = row['user']\n",
    "    text_hash = md5(text.encode()).hexdigest()\n",
    "    text_entity_predictions = get_ner_predictions(text, ner_model, label='GPE')\n",
    "    if text_entity_predictions:\n",
    "        predicted_entities += text_entity_predictions\n",
    "        users += [user] * len(text_entity_predictions)\n",
    "        hashes += [text_hash] * len(text_entity_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98725726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe and persist data\n",
    "transformed_reddit_data = pd.DataFrame({'user': users, \n",
    "                                        'predicted_entity': predicted_entities,\n",
    "                                        'hash': hashes})\n",
    "transformed_reddit_data.to_csv(\"transformed_reddit_data_nyc_2021_01_17.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c42c01c",
   "metadata": {},
   "source": [
    "# Load Data into SQL Database\n",
    "\n",
    "Browse your a local Sqlite database using [DBrowser](https://sqlitebrowser.org/dl/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1799c8",
   "metadata": {},
   "source": [
    "## Extracted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff468285",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('reddit.db')\n",
    "cursor = con.cursor()\n",
    "print(\"Successfully Connected to Database\")\n",
    "\n",
    "# Drop table if already exists\n",
    "cursor.execute(\"DROP TABLE IF EXISTS comments\")\n",
    "con.commit()\n",
    "\n",
    "# Create Table\n",
    "sql_create_table = \"\"\"CREATE TABLE IF NOT EXISTS comments (\n",
    "                                    table_id integer PRIMARY KEY,\n",
    "                                    user text NOT NULL,\n",
    "                                    comment text NOT NULL,\n",
    "                                    hash text NOT NULL,\n",
    "                                    time integer NOT NULL\n",
    "                                );\"\"\"\n",
    "cursor.execute(sql_create_table)\n",
    "con.commit()\n",
    "print(\"Succesfully create comments table\")\n",
    "\n",
    "# Insert rows from extracted entity dataframe\n",
    "for i, row in tqdm(extracted_reddit_data.iterrows(), \n",
    "                   total=len(extracted_reddit_data)):\n",
    "    sqlite_insert_query = f\"\"\"INSERT INTO comments\n",
    "                          (table_id, user, comment, hash, time) \n",
    "                           VALUES \n",
    "                          ('{i}', '{row['user']}', \n",
    "                          '{row['comment'].replace(\"'\", \"''\")}', '{row['hash']}', '{row['time']}')\"\"\"\n",
    "\n",
    "    cursor.execute(sqlite_insert_query)\n",
    "    con.commit()\n",
    "print(f\"Successfully inserted {len(extracted_reddit_data)} rows into the comments table\")\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb896865",
   "metadata": {},
   "source": [
    "## Predicted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03eab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('reddit.db')\n",
    "cursor = con.cursor()\n",
    "print(\"Successfully Connected to Database\")\n",
    "\n",
    "# Drop table if already exists\n",
    "cursor.execute(\"DROP TABLE IF EXISTS entity_predictions\")\n",
    "con.commit()\n",
    "\n",
    "# Create Table\n",
    "sql_create_table = \"\"\"CREATE TABLE IF NOT EXISTS entity_predictions (\n",
    "                                    table_id integer PRIMARY KEY,\n",
    "                                    user text NOT NULL,\n",
    "                                    predicted_entity text NOT NULL,\n",
    "                                    hash text NOT NULL\n",
    "                                );\"\"\"\n",
    "cursor.execute(sql_create_table)\n",
    "con.commit()\n",
    "print(\"Succesfully create entity_predictions table\")\n",
    "\n",
    "# Insert rows from extracted entity dataframe\n",
    "for i, row in tqdm(transformed_reddit_data.iterrows(), \n",
    "                   total=len(transformed_reddit_data)):\n",
    "    sqlite_insert_query = f\"\"\"INSERT INTO entity_predictions\n",
    "                          (table_id, user, predicted_entity, hash) \n",
    "                           VALUES \n",
    "                          ('{i}', '{row['user']}', '{row['predicted_entity'].replace(\"'\", \"''\")}', '{row['hash']}')\"\"\"\n",
    "\n",
    "    cursor.execute(sqlite_insert_query)\n",
    "    con.commit()\n",
    "print(f\"Successfully inserted {len(transformed_reddit_data)} rows into the entity_predictions table\")\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab7f573",
   "metadata": {},
   "source": [
    "# Connected Components Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('reddit.db')\n",
    "cursor = con.cursor()\n",
    "\n",
    "# Drop table if already exists\n",
    "bushwick_comments_query = \"\"\"\n",
    "SELECT\n",
    "  c.user,\n",
    "  c.comment\n",
    "FROM entity_predictions ep\n",
    "JOIN comments c\n",
    "  ON ep.hash = c.hash\n",
    "WHERE ep.predicted_entity = 'Bushwick'\n",
    "\"\"\"\n",
    "bushwick_mentions = pd.read_sql(bushwick_comments_query, con=con)\n",
    "cursor.close()\n",
    "for _, row in bushwick_mentions.iterrows():\n",
    "    print(f\"User: {row['user']}\")\n",
    "    print(f\"Comment: {row['comment']}\")\n",
    "    print(\"-\" * 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
